{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8b619af",
   "metadata": {},
   "source": [
    "# Data Science Jobs - API Scraping Notebook\n",
    "\n",
    "## Project: Data Science Job Market Analysis\n",
    "**Author:** Mayenmein Terence Sama Aloah Jr<br>\n",
    "**Date:** 09/23/2025  \n",
    "**Description:** This notebook handles the API-based scraping of data science job postings from Found.dev API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f5552ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "# Add src to path\n",
    "sys.path.insert(0, '..')\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b11aa0",
   "metadata": {},
   "source": [
    "## 1. Project Setup and Configuration\n",
    "Configure paths and import the scraping module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "368896bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Data will be saved to: c:\\Users\\MARIE\\Desktop\\scrape job details\\notebooks\\..\\data\\raw\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "DATA_RAW_PATH = Path('../data/raw')\n",
    "\n",
    "# Create directories if they don't exist\n",
    "DATA_RAW_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"📁 Data will be saved to: {DATA_RAW_PATH.absolute()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22a5d47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Custom scraping modules imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import the scraping function\n",
    "try:\n",
    "    from scr.scraping.scrape_jobs import scrape_in_batches, fetch_jobs\n",
    "    print(\"✅ Custom scraping modules imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Error importing custom modules: {e}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7430518e",
   "metadata": {},
   "source": [
    "## 2. API Connection Test\n",
    "Test the API connection with a single page request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d115df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing API connection...\n",
      "✅ API connection successful! Found 100 jobs on page 1\n",
      "\n",
      "📋 Sample job structure:\n",
      "{\n",
      "  \"job\": {\n",
      "    \"created_at\": \"0001-01-01T00:00:00Z\",\n",
      "    \"premium\": false,\n",
      "    \"title\": \"AI Testing Lead\",\n",
      "    \"roles\": \"\",\n",
      "    \"skills\": \"UI,Data Science,Conversational AI,Product Management,Product Design,UX\",\n",
      "    \"soft_skills\": \"\",\n",
      "    \"tools\": \"\",\n",
      "    \"languages\": \"\",\n",
      "    \"frameworks\": \"\",\n",
      "    \"libraries\": \"\",\n",
      "    \"seniority\": \"\",\n",
      "    \"type\": \"Full Time\",\n",
      "    \"city\": \"San Bruno, CA\",\n",
      "    \"country\": \"USA\",\n",
      "    \"published\": \"2025-09-23T15:43:53.924045Z\",\n",
      "    \"pin_until\": \"0001-01-01T00:00:00...\n"
     ]
    }
   ],
   "source": [
    "# Test API connection\n",
    "print(\"🧪 Testing API connection...\")\n",
    "\n",
    "try:\n",
    "    test_data = fetch_jobs(page=1, skill=\"Data Science\", ai=True)\n",
    "    jobs_count = len(test_data.get(\"jobs\", []))\n",
    "    print(f\"✅ API connection successful! Found {jobs_count} jobs on page 1\")\n",
    "    \n",
    "    # Display sample job structure\n",
    "    if jobs_count > 0:\n",
    "        sample_job = test_data[\"jobs\"][0]\n",
    "        print(\"\\n📋 Sample job structure:\")\n",
    "        print(json.dumps(sample_job, indent=2)[:500] + \"...\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ API test failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3c9477",
   "metadata": {},
   "source": [
    "## 3. Scraping Parameters Configuration\n",
    "Configure the scraping parameters for the batch process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "573fe22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙️  Scraping Configuration:\n",
      "   skill: Data Science\n",
      "   pages_per_batch: 20\n",
      "   ai: True\n",
      "   delay: 1\n",
      "   start_page: 41\n",
      "   start_batch: 1\n"
     ]
    }
   ],
   "source": [
    "# Scraping configuration\n",
    "SCRAPING_CONFIG = {\n",
    "    \"skill\": \"Data Science\",\n",
    "    \"pages_per_batch\": 20,\n",
    "    \"ai\": True,\n",
    "    \"delay\": 1,\n",
    "    \"start_page\": 41,\n",
    "    \"start_batch\": 1\n",
    "}\n",
    "\n",
    "print(\"⚙️  Scraping Configuration:\")\n",
    "for key, value in SCRAPING_CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03302fce",
   "metadata": {},
   "source": [
    "## 4. Execute Batch Scraping\n",
    "Run the main scraping process to collect job data in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e5315e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting batch scraping process...\n",
      "⏳ This may take several minutes depending on the number of pages...\n",
      "🕐 Started at: 2025-09-23 17:37:29\n",
      "\n",
      "🚀 Starting batch 3 (pages 41 → 60)\n",
      "Fetching page 41...\n",
      "Fetching page 42...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🕐 Started at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_time\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 9\u001b[0m     total_jobs_collected \u001b[38;5;241m=\u001b[39m \u001b[43mscrape_in_batches\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSCRAPING_CONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mskill\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpages_per_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSCRAPING_CONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpages_per_batch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mai\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSCRAPING_CONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mai\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSCRAPING_CONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdelay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m     17\u001b[0m     duration \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\MARIE\\Desktop\\scrape job details\\notebooks\\..\\scr\\scraping\\scrape_jobs.py:68\u001b[0m, in \u001b[0;36mscrape_in_batches\u001b[1;34m(skill, pages_per_batch, ai, delay)\u001b[0m\n\u001b[0;32m     66\u001b[0m     total_jobs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(jobs)\n\u001b[0;32m     67\u001b[0m     page \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 68\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Save after every batch\u001b[39;00m\n\u001b[0;32m     71\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(all_jobs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Start the batch scraping process\n",
    "print(\"🚀 Starting batch scraping process...\")\n",
    "print(\"⏳ This may take several minutes depending on the number of pages...\")\n",
    "\n",
    "start_time = datetime.now()\n",
    "print(f\"🕐 Started at: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "try:\n",
    "    total_jobs_collected = scrape_in_batches(\n",
    "        skill=SCRAPING_CONFIG[\"skill\"],\n",
    "        pages_per_batch=SCRAPING_CONFIG[\"pages_per_batch\"],\n",
    "        ai=SCRAPING_CONFIG[\"ai\"],\n",
    "        delay=SCRAPING_CONFIG[\"delay\"]\n",
    "    )\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    print(f\"\\n🎉 Scraping completed successfully!\")\n",
    "    print(f\"📊 Total jobs collected: {total_jobs_collected}\")\n",
    "    print(f\"⏱️  Duration: {duration}\")\n",
    "    print(f\"🕐 Finished at: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Scraping failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01f24f4",
   "metadata": {},
   "source": [
    "## 5. Data Quality Check\n",
    "Verify the collected data and check for any issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c643a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Found 6 batch files:\n",
      "   jobs_batch_1.csv: 2000 jobs\n",
      "   jobs_batch_2.csv: 1999 jobs\n",
      "   jobs_batch_3.csv: 2000 jobs\n",
      "   jobs_batch_4.csv: 2000 jobs\n",
      "   jobs_batch_5.csv: 2000 jobs\n",
      "   jobs_batch_6.csv: 1625 jobs\n",
      "\n",
      "📊 Sample from first batch (2000 jobs):\n",
      "   Columns: ['title', 'company', 'city', 'country', 'location', 'skills', 'type', 'salary', 'salary_min', 'salary_max', 'published', 'ai']\n",
      "   Date range: 2025-09-03T15:31:25.707176Z to 2025-09-23T12:23:09.98934Z\n",
      "\n",
      "📋 First 3 job titles:\n",
      "   1. Associate Research Director - Artificial Intelligence & Machine Learning\n",
      "   2. ML Engineer\n",
      "   3. Applied Scientist - Deep Learning\n"
     ]
    }
   ],
   "source": [
    "# Check what batch files were created\n",
    "batch_files = list(DATA_RAW_PATH.glob(\"jobs_batch_*.csv\"))\n",
    "print(f\"📁 Found {len(batch_files)} batch files:\")\n",
    "\n",
    "for batch_file in sorted(batch_files):\n",
    "    df = pd.read_csv(batch_file)\n",
    "    print(f\"   {batch_file.name}: {len(df)} jobs\")\n",
    "\n",
    "# Load and examine the first batch\n",
    "if batch_files:\n",
    "    first_batch = pd.read_csv(batch_files[0])\n",
    "    print(f\"\\n📊 Sample from first batch ({len(first_batch)} jobs):\")\n",
    "    print(f\"   Columns: {list(first_batch.columns)}\")\n",
    "    print(f\"   Date range: {first_batch['published'].min()} to {first_batch['published'].max()}\")\n",
    "    \n",
    "    # Display first few rows\n",
    "    print(\"\\n📋 First 3 job titles:\")\n",
    "    for i, title in enumerate(first_batch['title'].head(3)):\n",
    "        print(f\"   {i+1}. {title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c00114",
   "metadata": {},
   "source": [
    "## 6. Data Summary Statistics\n",
    "Generate basic statistics about the collected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "995b426c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Data Summary Statistics:\n",
      "Total jobs collected: 11624\n",
      "Unique companies: 1533\n",
      "Unique locations: 912\n",
      "Date range: 2023-10-23T14:23:55.119Z to 2025-09-23T12:23:09.98934Z\n",
      "\n",
      "📊 Job Type Distribution:\n",
      "   Full Time: 7777 jobs (66.9%)\n",
      "   Remote,Full Time: 735 jobs (6.3%)\n",
      "   Remote: 677 jobs (5.8%)\n",
      "   Temporary: 644 jobs (5.5%)\n",
      "   Intern: 281 jobs (2.4%)\n",
      "   Freelancer: 174 jobs (1.5%)\n",
      "   Vollzeit: 129 jobs (1.1%)\n",
      "   Stage: 116 jobs (1.0%)\n",
      "   Voltijds: 100 jobs (0.9%)\n",
      "   Onsite: 86 jobs (0.7%)\n",
      "   Remote,Full Time,Freelancer: 59 jobs (0.5%)\n",
      "   Remote,Freelancer: 57 jobs (0.5%)\n",
      "   Part Time: 56 jobs (0.5%)\n",
      "   Full Time,Freelancer: 51 jobs (0.4%)\n",
      "   Permanent: 47 jobs (0.4%)\n",
      "   Internship: 46 jobs (0.4%)\n",
      "   Remote,Internship: 39 jobs (0.3%)\n",
      "   Full Time,Onsite: 38 jobs (0.3%)\n",
      "   Full Time,Internship: 35 jobs (0.3%)\n",
      "   Part Time,Full Time: 35 jobs (0.3%)\n",
      "   Remote,Onsite: 28 jobs (0.2%)\n",
      "   Hybrid: 26 jobs (0.2%)\n",
      "   Employee: 25 jobs (0.2%)\n",
      "   Heltid: 17 jobs (0.1%)\n",
      "   A jornada completa: 15 jobs (0.1%)\n",
      "   Praktikant/in: 15 jobs (0.1%)\n",
      "   Freelancer,Temporary: 13 jobs (0.1%)\n",
      "   Teilzeit: 12 jobs (0.1%)\n",
      "   Pełny etat: 11 jobs (0.1%)\n",
      "   Stagiair: 10 jobs (0.1%)\n",
      "   Freelancer,Onsite: 10 jobs (0.1%)\n",
      "   Remote,Part Time: 8 jobs (0.1%)\n",
      "   Remote,Full Time,Onsite: 8 jobs (0.1%)\n",
      "   Regular: 7 jobs (0.1%)\n",
      "   Remote,Full Time,Internship: 7 jobs (0.1%)\n",
      "   Remote,Part Time,Full Time: 7 jobs (0.1%)\n",
      "   Contrat: 7 jobs (0.1%)\n",
      "   On-site: 7 jobs (0.1%)\n",
      "   Temporary,Onsite: 7 jobs (0.1%)\n",
      "   Remote,Freelancer,Internship: 5 jobs (0.0%)\n",
      "   Internship,Onsite: 5 jobs (0.0%)\n",
      "   Teljes munkaidő: 4 jobs (0.0%)\n",
      "   Fulltid: 4 jobs (0.0%)\n",
      "   Full Time,Freelancer,Internship: 4 jobs (0.0%)\n",
      "   Full Time,Temporary: 4 jobs (0.0%)\n",
      "   Part Time,Full Time,Onsite: 3 jobs (0.0%)\n",
      "   Part Time,Full Time,Temporary: 3 jobs (0.0%)\n",
      "   Remote,Freelancer,Onsite: 3 jobs (0.0%)\n",
      "   Part Time,Full Time,Internship: 3 jobs (0.0%)\n",
      "   Remote,Freelancer,Temporary: 3 jobs (0.0%)\n",
      "   Gyakornok: 3 jobs (0.0%)\n",
      "   Part Time,Internship: 3 jobs (0.0%)\n",
      "   Remote,Part Time,Full Time,Freelancer: 3 jobs (0.0%)\n",
      "   Part Time,Temporary: 3 jobs (0.0%)\n",
      "   Remote,Full Time,Freelancer,Temporary: 3 jobs (0.0%)\n",
      "   Remote,Part Time,Full Time,Onsite: 3 jobs (0.0%)\n",
      "   Full Time,Freelancer,Temporary: 3 jobs (0.0%)\n",
      "   Remote,Temporary: 3 jobs (0.0%)\n",
      "   Full Time,Internship,Onsite: 3 jobs (0.0%)\n",
      "   Deeltijds: 3 jobs (0.0%)\n",
      "   Graduate,Hybrid: 2 jobs (0.0%)\n",
      "   Data Analysts &amp; Reporters: 2 jobs (0.0%)\n",
      "   On-roll: 2 jobs (0.0%)\n",
      "   Remote,Full Time,Internship,Temporary,Onsite: 2 jobs (0.0%)\n",
      "   Part Time,Full Time,Freelancer,Onsite: 2 jobs (0.0%)\n",
      "   Full - Time: 2 jobs (0.0%)\n",
      "   CDI: 2 jobs (0.0%)\n",
      "   Fuldtid: 2 jobs (0.0%)\n",
      "   on-site: 2 jobs (0.0%)\n",
      "   Full Time,Internship,Temporary: 2 jobs (0.0%)\n",
      "   Regular,Analytics & Modelling,Analytics & Data Science: 2 jobs (0.0%)\n",
      "   Binance Accelerator Program: 2 jobs (0.0%)\n",
      "   Part Time,Freelancer: 2 jobs (0.0%)\n",
      "   Hybrid.: 2 jobs (0.0%)\n",
      "   Werkvertrag: 2 jobs (0.0%)\n",
      "   Billable: 1 jobs (0.0%)\n",
      "   Remote,Part Time,Freelancer: 1 jobs (0.0%)\n",
      "   Apprenticeship: 1 jobs (0.0%)\n",
      "   Interns: 1 jobs (0.0%)\n",
      "   In-person: 1 jobs (0.0%)\n",
      "   Client-facing,: 1 jobs (0.0%)\n",
      "   Employee - Regular/Permanent: 1 jobs (0.0%)\n",
      "   Aprio Colombia: 1 jobs (0.0%)\n",
      "   On-site.: 1 jobs (0.0%)\n",
      "   Полная занятость: 1 jobs (0.0%)\n",
      "   正規雇用: 1 jobs (0.0%)\n",
      "   On-rolls: 1 jobs (0.0%)\n",
      "   Software &amp; Database Engineers: 1 jobs (0.0%)\n",
      "   Частичная занятость: 1 jobs (0.0%)\n",
      "   Regular,on-site: 1 jobs (0.0%)\n",
      "   FT: 1 jobs (0.0%)\n",
      "   Technical Sales: 1 jobs (0.0%)\n",
      "   Stagista: 1 jobs (0.0%)\n",
      "   Full Time,Freelancer,Internship,Temporary: 1 jobs (0.0%)\n",
      "   Application/Software Developer: 1 jobs (0.0%)\n",
      "   Regular Employee: 1 jobs (0.0%)\n",
      "   Remote,Internship,Onsite: 1 jobs (0.0%)\n",
      "   Intern,Co-Op: 1 jobs (0.0%)\n",
      "   on-site,hybrid: 1 jobs (0.0%)\n",
      "   Remote,Full Time,Internship,Onsite: 1 jobs (0.0%)\n",
      "   Data Analyst: 1 jobs (0.0%)\n",
      "   Full Time,Internship,Temporary,Onsite: 1 jobs (0.0%)\n",
      "   Co-op,Intern,Hybrid: 1 jobs (0.0%)\n",
      "   Remote,Temporary,Onsite: 1 jobs (0.0%)\n",
      "   Intern,Summer: 1 jobs (0.0%)\n",
      "   Specialist - employee: 1 jobs (0.0%)\n",
      "   Hybrid,Regular: 1 jobs (0.0%)\n",
      "   Internship,Temporary: 1 jobs (0.0%)\n",
      "   Part Time,Onsite: 1 jobs (0.0%)\n",
      "   Freelancer,Temporary,Onsite: 1 jobs (0.0%)\n",
      "   Remote,Part Time,Full Time,Temporary: 1 jobs (0.0%)\n",
      "   Regular Sales Operations Sales: 1 jobs (0.0%)\n",
      "   Part Time,Full Time,Freelancer: 1 jobs (0.0%)\n",
      "   Remote,Part Time,Full Time,Freelancer,Temporary: 1 jobs (0.0%)\n",
      "   CLT: 1 jobs (0.0%)\n",
      "   GIS,on-site: 1 jobs (0.0%)\n",
      "   Full Time,Freelancer,Onsite: 1 jobs (0.0%)\n",
      "   Remote,Full Time,Freelancer,Internship: 1 jobs (0.0%)\n",
      "   In-office,hybrid: 1 jobs (0.0%)\n",
      "   Hybrid,Graduate Programme: 1 jobs (0.0%)\n",
      "   Intern,Hybrid: 1 jobs (0.0%)\n",
      "   Professional,Entry Professional: 1 jobs (0.0%)\n",
      "   Hybrid,Office-based: 1 jobs (0.0%)\n",
      "   Executive: 1 jobs (0.0%)\n",
      "   FTE or 1099: 1 jobs (0.0%)\n",
      "   Working Student: 1 jobs (0.0%)\n",
      "\n",
      "🤖 AI-related jobs: 11624 (100.0%)\n"
     ]
    }
   ],
   "source": [
    "# Combine all batches for summary statistics\n",
    "if batch_files:\n",
    "    all_data = pd.concat([pd.read_csv(f) for f in batch_files], ignore_index=True)\n",
    "    \n",
    "    print(\"📈 Data Summary Statistics:\")\n",
    "    print(f\"Total jobs collected: {len(all_data)}\")\n",
    "    print(f\"Unique companies: {all_data['company'].nunique()}\")\n",
    "    print(f\"Unique locations: {all_data['location'].nunique()}\")\n",
    "    print(f\"Date range: {all_data['published'].min()} to {all_data['published'].max()}\")\n",
    "    \n",
    "    # Job type distribution\n",
    "    print(\"\\n📊 Job Type Distribution:\")\n",
    "    job_type_counts = all_data['type'].value_counts()\n",
    "    for job_type, count in job_type_counts.items():\n",
    "        percentage = (count / len(all_data)) * 100\n",
    "        print(f\"   {job_type}: {count} jobs ({percentage:.1f}%)\")\n",
    "    \n",
    "    # AI-related jobs\n",
    "    ai_jobs = all_data['ai'].sum() if 'ai' in all_data.columns else 0\n",
    "    print(f\"\\n🤖 AI-related jobs: {ai_jobs} ({ (ai_jobs/len(all_data)*100 ):.1f}%)\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ No batch files found for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157d932c",
   "metadata": {},
   "source": [
    "## 7. Data Validation\n",
    "Check for data quality issues and missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "614dc952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Data Quality Check:\n",
      "\n",
      "❌ Missing values per column:\n",
      "   city: 2597 missing (22.3%)\n",
      "   country: 569 missing (4.9%)\n",
      "   type: 50 missing (0.4%)\n",
      "   salary: 7956 missing (68.4%)\n",
      "\n",
      "🔄 Duplicate entries: 28\n",
      "\n",
      "📝 Data types:\n",
      "title         object\n",
      "company       object\n",
      "city          object\n",
      "country       object\n",
      "location      object\n",
      "skills        object\n",
      "type          object\n",
      "salary        object\n",
      "salary_min     int64\n",
      "salary_max     int64\n",
      "published     object\n",
      "ai              bool\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Data quality check\n",
    "if batch_files:\n",
    "    print(\"🔍 Data Quality Check:\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_data = all_data.isnull().sum()\n",
    "    print(\"\\n❌ Missing values per column:\")\n",
    "    for column, missing_count in missing_data.items():\n",
    "        if missing_count > 0:\n",
    "            percentage = (missing_count / len(all_data)) * 100\n",
    "            print(f\"   {column}: {missing_count} missing ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Check for duplicates\n",
    "    duplicates = all_data.duplicated().sum()\n",
    "    print(f\"\\n🔄 Duplicate entries: {duplicates}\")\n",
    "    \n",
    "    # Check data types\n",
    "    print(f\"\\n📝 Data types:\")\n",
    "    print(all_data.dtypes)\n",
    "\n",
    "else:\n",
    "    print(\"❌ No data available for validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f131467b",
   "metadata": {},
   "source": [
    "## 8. Save Metadata and Logs\n",
    "Record scraping session information for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4268859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Metadata saved to: ..\\data\\raw\\scraping_metadata.json\n"
     ]
    }
   ],
   "source": [
    "# Save scraping metadata\n",
    "metadata = {\n",
    "    \"scraping_session\": {\n",
    "        \"start_time\": start_time.isoformat(),\n",
    "        \"end_time\": end_time.isoformat() if 'end_time' in locals() else datetime.now().isoformat(),\n",
    "        \"duration_seconds\": duration.total_seconds() if 'duration' in locals() else 0,\n",
    "        \"total_jobs_collected\": total_jobs_collected if 'total_jobs_collected' in locals() else 0,\n",
    "        \"batches_created\": len(batch_files),\n",
    "        \"configuration\": SCRAPING_CONFIG\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save metadata to file\n",
    "metadata_path = DATA_RAW_PATH / \"scraping_metadata.json\"\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"✅ Metadata saved to: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa9666d",
   "metadata": {},
   "source": [
    "## 9. Next Steps\n",
    "Prepare for the next phase of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe853ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🎯 Next Steps:\")\n",
    "print(\"1. ✅ Data scraping completed\")\n",
    "print(\"2. ➡️  Proceed to 02_cleaning.ipynb for data cleaning\")\n",
    "print(\"3. 🔧 Clean the raw data and handle missing values\")\n",
    "print(\"4. 💾 Save cleaned data to data/interim/ directory\")\n",
    "\n",
    "# Display file sizes for reference\n",
    "if batch_files:\n",
    "    print(f\"\\n📊 File sizes:\")\n",
    "    for batch_file in sorted(batch_files):\n",
    "        size_mb = os.path.getsize(batch_file) / (1024 * 1024)\n",
    "        print(f\"   {batch_file.name}: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53978470",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- **API Source**: Found.dev jobs API\n",
    "- **Data Collected**: Data Science job postings\n",
    "- **Output**: Multiple CSV batches in `data/raw/` directory\n",
    "- **Next Phase**: Data cleaning and preprocessing\n",
    "- **Testing**: Unit tests are maintained separately in `tests/test_scraping.py`\n",
    "\n",
    "The scraping process is complete and the data is ready for the next stage of the pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
